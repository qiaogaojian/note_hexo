---
title: Q-learning
date: 2024-10-21 16:13:24
categories: ['5.æŠ€èƒ½', 'AI', '_What_']
tags: ['What', 'AI', 'æŠ€èƒ½', 'srcard']
---
  
  
## æ¦‚å¿µ

  
**Q-learning** æ˜¯ä¸€ç§ç»å…¸çš„ **[å¼ºåŒ–å­¦ä¹ ](../689f27ebabe459360038ee0e75643af26f2e718a)ï¼ˆReinforcement Learning, RLï¼‰** ç®—æ³•ï¼Œå®ƒå±äºåŸºäºå€¼å‡½æ•°çš„å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¸®åŠ©æ™ºèƒ½ä½“åœ¨ç»™å®šç¯å¢ƒä¸­é€šè¿‡å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼ˆPolicyï¼‰æ¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learning çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ™ºèƒ½ä½“é€šè¿‡åå¤ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œä¼°è®¡æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„â€œä»·å€¼â€ï¼Œå¹¶é€æ­¥æ”¹è¿›å†³ç­–ï¼Œæœ€ç»ˆæ‰¾åˆ°æœ€ä¼˜åŠ¨ä½œç­–ç•¥ã€‚
  
  
### Q-learningçš„æ ¸å¿ƒæ¦‚å¿µ

1. **Qå€¼ï¼ˆQ-valueï¼‰**:
   Q-learning ä½¿ç”¨ä¸€ä¸ªç§°ä¸º Q å€¼ï¼ˆåˆç§°ä¸ºçŠ¶æ€-åŠ¨ä½œå€¼ï¼‰çš„å‡½æ•°æ¥è¡¨ç¤ºæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼ã€‚Q å€¼å®šä¹‰ä¸ºåœ¨ç»™å®šçŠ¶æ€ \( s \) ä¸‹ï¼Œæ‰§è¡ŒæŸä¸ªåŠ¨ä½œ \( a \) åèƒ½è·å¾—çš„é¢„æœŸç´¯ç§¯å¥–åŠ±ï¼ˆåŒ…æ‹¬å½“å‰å¥–åŠ±å’Œæœªæ¥çš„æŠ˜æ‰£å¥–åŠ±ï¼‰ã€‚Q å€¼å‡½æ•°é€šå¸¸è®°ä½œ \( Q(s, a) \)ï¼Œå…¶ä¸­ï¼š
   - \( s \) è¡¨ç¤ºå½“å‰çš„çŠ¶æ€ã€‚
   - \( a \) è¡¨ç¤ºå½“å‰çš„åŠ¨ä½œã€‚

2. **è´å°”æ›¼æ–¹ç¨‹ï¼ˆBellman Equationï¼‰**:
   Q-learning åŸºäºè´å°”æ›¼æ–¹ç¨‹æ›´æ–° Q å€¼ï¼Œå³æœªæ¥çš„é¢„æœŸå¥–åŠ±ç­‰äºå½“å‰å¥–åŠ±åŠ ä¸Šä»ä¸‹ä¸€ä¸ªçŠ¶æ€å¼€å§‹çš„æœ€ä¼˜åŠ¨ä½œå¸¦æ¥çš„æŠ˜æ‰£ç´¯ç§¯å¥–åŠ±ã€‚Q-learning é€šè¿‡å¯¹è´å°”æ›¼æ–¹ç¨‹è¿›è¡Œè¿­ä»£æ›´æ–°æ¥é€¼è¿‘æœ€ä¼˜ Q å€¼ã€‚

3. **åŠ¨ä½œé€‰æ‹©ç­–ç•¥**:
   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨æ¢ç´¢æ–°åŠ¨ä½œå’Œåˆ©ç”¨å·²çŸ¥æœ€ä¼˜åŠ¨ä½œä¹‹é—´åšå‡ºæƒè¡¡ï¼Œç§°ä¸º**æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼ˆExploration vs. Exploitation Tradeoffï¼‰**ã€‚
   - **æ¢ç´¢ï¼ˆExplorationï¼‰**: å°è¯•æ–°çš„æˆ–éšæœºçš„åŠ¨ä½œï¼Œä»¥å‘ç°æ½œåœ¨çš„æ›´é«˜æ”¶ç›Šã€‚
   - **åˆ©ç”¨ï¼ˆExploitationï¼‰**: æ ¹æ®å½“å‰å·²çŸ¥çš„ Q å€¼é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œä»¥è·å–æœ€å¤§å¥–åŠ±ã€‚
   å¸¸ç”¨çš„ç­–ç•¥æ˜¯**Îµ-greedyç­–ç•¥**ï¼Œå³æ™ºèƒ½ä½“ä»¥ä¸€å®šæ¦‚ç‡ï¼ˆÎµï¼‰éšæœºé€‰æ‹©åŠ¨ä½œè¿›è¡Œæ¢ç´¢ï¼Œä»¥ \( 1 - Îµ \) çš„æ¦‚ç‡é€‰æ‹©å½“å‰ä¼°è®¡çš„æœ€ä¼˜åŠ¨ä½œè¿›è¡Œåˆ©ç”¨ã€‚
  
  
### Q-learningçš„ç®—æ³•æ­¥éª¤

Q-learning çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š

1. **åˆå§‹åŒ–**: 
   åˆå§‹åŒ– Q å€¼è¡¨ \( Q(s, a) \)ï¼Œä¸ºæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼ˆé€šå¸¸ä¸º 0 æˆ–éšæœºå€¼ï¼‰ã€‚è¯¥è¡¨ç”¨äºå­˜å‚¨æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„å½“å‰ä¼°è®¡ä»·å€¼ã€‚

2. **å¾ªç¯è¿‡ç¨‹**ï¼ˆç›´åˆ°æ”¶æ•›ï¼‰:
   å¯¹äºæ¯ä¸ªå›åˆæˆ–æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
   
   - **çŠ¶æ€è§‚å¯Ÿ**: æ™ºèƒ½ä½“è§‚å¯Ÿå½“å‰çš„çŠ¶æ€ \( s \)ã€‚
   
   - **é€‰æ‹©åŠ¨ä½œ**: æ ¹æ® Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ \( a \)ï¼Œå³éšæœºé€‰æ‹©æˆ–é€‰æ‹©æœ€å¤§ Q å€¼å¯¹åº”çš„åŠ¨ä½œã€‚
   
   - **æ‰§è¡ŒåŠ¨ä½œ**: åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ \( a \)ï¼Œå¾—åˆ°å³æ—¶å¥–åŠ± \( r \)ï¼Œå¹¶è§‚å¯Ÿæ–°çš„çŠ¶æ€ \( s' \)ã€‚
   
   - **æ›´æ–° Q å€¼**: æ ¹æ®è´å°”æ›¼æ–¹ç¨‹æ›´æ–° Q å€¼ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹ï¼š
     $$
     Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
     $$
     å…¶ä¸­ï¼š
     - $\alpha$æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶ Q å€¼æ›´æ–°çš„æ­¥é•¿ã€‚
     - $\gamma$ æ˜¯æŠ˜æ‰£å› å­ï¼Œæƒè¡¡å½“å‰å¥–åŠ±å’Œæœªæ¥å¥–åŠ±çš„é‡è¦æ€§ã€‚
     - $\max_{a'} Q(s', a')$ æ˜¯åœ¨æ–°çŠ¶æ€ $s'$ ä¸‹çš„æœ€ä¼˜åŠ¨ä½œæ‰€å¯¹åº”çš„ Q å€¼ã€‚
   
   - **çŠ¶æ€æ›´æ–°**: å°†æ–°çš„çŠ¶æ€ $s'$ ä½œä¸ºå½“å‰çŠ¶æ€ï¼Œé‡å¤ä¸Šè¿°æ­¥éª¤ã€‚

3. **ç»ˆæ­¢æ¡ä»¶**: è¯¥è¿‡ç¨‹ä¼šä¸€ç›´é‡å¤ï¼Œç›´åˆ° Q å€¼æ”¶æ•›ï¼ˆå³ Q å€¼å˜åŒ–è¶‹äºç¨³å®šï¼‰æˆ–è¾¾åˆ°é¢„è®¾çš„è®­ç»ƒè½®æ¬¡ã€‚
  
  
### Q-learningçš„ä¼˜ç‚¹

1. **ç®€å•æ˜“å®ç°**: Q-learning æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•ä¸”æ˜“äºå®ç°çš„ç®—æ³•ï¼Œå®ƒé€šè¿‡è¡¨æ ¼å½¢å¼å­˜å‚¨æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„ Q å€¼ï¼Œé€‚åˆç¦»æ•£çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„é—®é¢˜ã€‚

2. **æ— æ¨¡å‹æ–¹æ³•ï¼ˆModel-Freeï¼‰**: Q-learning ä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„åŠ¨æ€æ¨¡å‹ï¼ˆå¦‚è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°ï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºç»éªŒçš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼Œä»…ä¾èµ–æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ•°æ®ã€‚

3. **å…¨å±€æœ€ä¼˜**: åœ¨ç†è®ºä¸Šï¼ŒQ-learning èƒ½å¤Ÿåœ¨è¶³å¤Ÿçš„æ—¶é—´å’Œæ¢ç´¢ä¸‹æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå³æ‰¾åˆ°æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„åŠ¨ä½œé€‰æ‹©æ–¹å¼ã€‚
  
  
### Q-learningçš„å±€é™æ€§

1. **ç»´æ•°ç¾éš¾**: å½“çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´è¾ƒå¤§æ—¶ï¼ˆä¾‹å¦‚å›¾åƒæˆ–é«˜ç»´åº¦è¾“å…¥ï¼‰ï¼ŒQ-learning éœ€è¦ç»´æŠ¤ä¸€ä¸ªå·¨å¤§çš„ Q å€¼è¡¨ï¼Œå­˜å‚¨æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„å€¼ï¼Œè¿™å¯¼è‡´å†…å­˜éœ€æ±‚æ€¥å‰§å¢åŠ ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤æ‚é—®é¢˜ã€‚

2. **æ”¶æ•›é€Ÿåº¦æ…¢**: åœ¨æŸäº›å¤æ‚ç¯å¢ƒä¸­ï¼ŒQ-learning çš„æ”¶æ•›é€Ÿåº¦å¯èƒ½éå¸¸æ…¢ï¼Œå°¤å…¶æ˜¯åœ¨æ¢ç´¢ä¸åˆ©ç”¨ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚

3. **ä»…é€‚ç”¨äºç¦»æ•£åŠ¨ä½œç©ºé—´**: ä¼ ç»Ÿçš„ Q-learning åªé€‚ç”¨äºç¦»æ•£çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼Œå¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œéœ€è¦æ‰©å±•ç®—æ³•æˆ–å¼•å…¥å‡½æ•°é€¼è¿‘æ–¹æ³•ï¼ˆå¦‚æ·±åº¦ Q-learningï¼‰ã€‚
  
  
### æ·±åº¦Qå­¦ä¹ ï¼ˆDeep Q-Network, DQNï¼‰

ä¸ºäº†è§£å†³ Q-learning åœ¨é«˜ç»´çŠ¶æ€ç©ºé—´ä¸‹çš„æ‰©å±•æ€§é—®é¢˜ï¼Œ**æ·±åº¦ Q-learningï¼ˆ[[5.æŠ€èƒ½\AI\RL\_What_\DQN]]ï¼‰** è¢«æå‡ºï¼Œå®ƒå°†**æ·±åº¦å­¦ä¹ **ä¸ Q-learning ç»“åˆï¼Œåˆ©ç”¨**æ·±åº¦ç¥ç»ç½‘ç»œ**æ¥è¿‘ä¼¼ Q å€¼å‡½æ•°ã€‚DQN åœ¨æ¸¸æˆé¢†åŸŸï¼ˆå¦‚ Atari æ¸¸æˆï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸè§£å†³äº†é«˜ç»´å›¾åƒè¾“å…¥çš„é—®é¢˜ï¼Œæ¨åŠ¨äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å‘å±•ã€‚

DQN çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
1. **ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œ**: ä»£æ›¿ä¼ ç»Ÿçš„ Q å€¼è¡¨ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œä¼°è®¡ Q å€¼ã€‚
2. **ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰**: é€šè¿‡å­˜å‚¨è¿‡å»çš„ç»éªŒå¹¶éšæœºé‡‡æ ·ç”¨äºè®­ç»ƒï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼Œæ”¹å–„å­¦ä¹ ç¨³å®šæ€§ã€‚
3. **ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰**: å¼•å…¥ä¸€ä¸ªç›®æ ‡ç½‘ç»œæ¥è®¡ç®—ç›®æ ‡ Q å€¼ï¼Œå‡å°‘ç½‘ç»œæ›´æ–°è¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚
  
  
### æ€»ç»“

**Q-learning** æ˜¯ä¸€ç§ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡åå¤æ›´æ–°çŠ¶æ€-åŠ¨ä½œå€¼è¡¨æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚å®ƒåœ¨ç®€å•ã€ç¦»æ•£çš„ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é«˜ç»´çŠ¶æ€ç©ºé—´æˆ–è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ï¼Œå­˜åœ¨æ‰©å±•æ€§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæ·±åº¦ Q-learningï¼ˆDQNï¼‰ä½¿å¾— Q-learning èƒ½å¤Ÿå¤„ç†å¤æ‚çš„é«˜ç»´é—®é¢˜ï¼Œæå¤§åœ°æ‰©å±•äº†å…¶åº”ç”¨èŒƒå›´ã€‚
  
  
## ä»£ç ç¤ºä¾‹

```python
from src.ai.rl.config.q_config import QConfig
from src.ai.rl.agent.agent_q import QAgent
from src.ai.rl.utils import rl_utils
from src.ai.rl.utils.rl_utils import *

matplotlib.use('Agg')


def train(cfg):
    env_train = gym.make(cfg.env_name, render_mode="rgb_array")
    agent = QAgent(cfg)

    for episode in range(cfg.episode_num):
        current_state = discretize_state(env_train.reset()[0])

        while True:
            action = agent.act(current_state)

            next_state, next_reward, done, _, _ = env_train.step(action)
            next_state = discretize_state(next_state)

            agent.replay_buffer.add(current_state, action, next_reward, next_state, done)
            agent.learn()

            current_state = next_state

            if done:
                break

        if (episode + 1) % 1 == 0:
            if (episode + 1) % 1 == 0:
                agent.save(episode, max_keep=3)
                predict(cfg, episode)

        # æ¯æ¬¡æ›´æ–°ç½‘ç»œåå€¾å‘äºä½¿ç”¨ç½‘ç»œé¢„æµ‹çš„ action æ¢ç´¢ç¯å¢ƒ, æ¢ç´¢ç³»æ•°ä¼šè¶Šæ¥è¶Šå°
        if agent.cfg.epsilon > agent.cfg.epsilon_min:
            agent.cfg.epsilon = max(agent.cfg.epsilon_min, agent.cfg.epsilon * agent.cfg.epsilon_decay)

    env_train.close()


def predict(cfg, episode):
    cfg.is_train = False

    env_eval = gym.make(cfg.env_name, render_mode="human")
    agent = QAgent(cfg)

    reward_list = evaluate(env_eval, agent, episode)
    if (episode + 1) % 10 == 0 and len(reward_list) > 10:
        plot(reward_list, episode)


def evaluate(env_display, agent, episode):
    reward_list = []
    state = discretize_state(env_display.reset()[0])
    episode_reward = 0
    for t in range(10000):
        env_display.render()

        action = agent.predict(state)

        next_state, reward, done, _, _ = env_display.step(action)
        next_state = discretize_state(next_state)
        state = next_state

        episode_reward += reward

        if done:
            print(f"Episode {episode} finished after {t + 1} timesteps with total reward {episode_reward}")
            break
    reward_list.append(episode_reward)

    return reward_list


def plot(reward_list, episode=999):
    episodes_list = list(range(len(reward_list)))

    mv_return = rl_utils.moving_average(reward_list, 9)
    plt.plot(episodes_list, mv_return)
    plt.xlabel('Episodes')
    plt.ylabel('Returns')
    plt.title('Q on {}'.format(cfg.env_name))
    plt.show()

    plot_path = f'{FileUtils.get_project_path("output")}/q-{TimeUtils.get_cur_timestamp()}-{episode}.png'
    plt.savefig(plot_path)  # ä¿å­˜å›¾è¡¨ä¸ºPNGæ–‡ä»¶
    plt.close()  # å…³é—­å›¾è¡¨çª—å£ï¼Œé˜²æ­¢èµ„æºå ç”¨


# ç¦»æ•£åŒ–çŠ¶æ€
def discretize_state(state, discretize_num=19):
    """https://github.com/openai/gym/wiki/CartPole-v0
    [position, cart velocity, angle, pole velocity]
    """
    lower_bounds = [-2.4, -3.0, -0.5, -2.0]
    upper_bounds = [2.4, 3.0, 0.5, 2.0]
    # ratios = [(state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(state))]
    # new_state = [int(round((discretize_num * ratios[i]))) for i in range(len(state))]
    # new_state = [min(discretize_num, max(0, new_state[i])) for i in range(len(state))]
    # return tuple(new_state)

    ratios = []
    new_state = []
    # è®¡ç®—æ¯ä¸ªçŠ¶æ€ç»´åº¦çš„å½’ä¸€åŒ–æ¯”ä¾‹
    for i in range(len(state)):
        ratio = (state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i])
        ratios.append(ratio)

    # ç¦»æ•£åŒ–çŠ¶æ€å¹¶å››èˆäº”å…¥
    for i in range(len(state)):
        discretized_value = int(round(discretize_num * ratios[i]))
        new_state.append(discretized_value)

    # è¾¹ç•Œè°ƒæ•´
    for i in range(len(state)):
        new_state[i] = min(discretize_num - 1, max(0, new_state[i]))

    return tuple(new_state)


if __name__ == "__main__":
    args = Tools.parse_args()
    core = Core()
    core.init(env=args.env)
    Logger.instance().info('********************************* Q *********************************')

    cfg = QConfig()
    train(cfg)
    predict(cfg, 999)

```


**ç›¸å…³ç¬”è®°:**

- [æ·±åº¦å¼ºåŒ–å­¦ä¹ ](../fb2736e62d44c8a1f1c732650a20795e18086c1b)
- [å¼ºåŒ–å­¦ä¹ ](../689f27ebabe459360038ee0e75643af26f2e718a)

{% pullquote mindmap mindmap-md %}
- ğŸ”µ
  - [æ·±åº¦å¼ºåŒ–å­¦ä¹ ](../fb2736e62d44c8a1f1c732650a20795e18086c1b)
  - [å¼ºåŒ–å­¦ä¹ ](../689f27ebabe459360038ee0e75643af26f2e718a)
{% endpullquote %}