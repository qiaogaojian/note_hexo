---
title: Q-learning
date: 2024-10-21 16:13:24
categories: ['5.技能', 'AI', '_What_']
tags: ['What', 'AI', '技能', 'srcard']
---
  
  
## 概念

  
**Q-learning** 是一种经典的 **[强化学习](../689f27ebabe459360038ee0e75643af26f2e718a)（Reinforcement Learning, RL）** 算法，它属于基于值函数的学习方法，用于帮助智能体在给定环境中通过学习最优策略（Policy）来最大化累积奖励。Q-learning 的核心思想是智能体通过反复与环境的交互，估计每个状态-动作对的“价值”，并逐步改进决策，最终找到最优动作策略。
  
  
### Q-learning的核心概念

1. **Q值（Q-value）**:
   Q-learning 使用一个称为 Q 值（又称为状态-动作值）的函数来表示每个状态-动作对的价值。Q 值定义为在给定状态 \( s \) 下，执行某个动作 \( a \) 后能获得的预期累积奖励（包括当前奖励和未来的折扣奖励）。Q 值函数通常记作 \( Q(s, a) \)，其中：
   - \( s \) 表示当前的状态。
   - \( a \) 表示当前的动作。

2. **贝尔曼方程（Bellman Equation）**:
   Q-learning 基于贝尔曼方程更新 Q 值，即未来的预期奖励等于当前奖励加上从下一个状态开始的最优动作带来的折扣累积奖励。Q-learning 通过对贝尔曼方程进行迭代更新来逼近最优 Q 值。

3. **动作选择策略**:
   在训练过程中，智能体需要在探索新动作和利用已知最优动作之间做出权衡，称为**探索与利用的平衡（Exploration vs. Exploitation Tradeoff）**。
   - **探索（Exploration）**: 尝试新的或随机的动作，以发现潜在的更高收益。
   - **利用（Exploitation）**: 根据当前已知的 Q 值选择最优动作，以获取最大奖励。
   常用的策略是**ε-greedy策略**，即智能体以一定概率（ε）随机选择动作进行探索，以 \( 1 - ε \) 的概率选择当前估计的最优动作进行利用。
  
  
### Q-learning的算法步骤

Q-learning 的基本流程如下：

1. **初始化**: 
   初始化 Q 值表 \( Q(s, a) \)，为每个状态-动作对分配一个初始值（通常为 0 或随机值）。该表用于存储每个状态-动作对的当前估计价值。

2. **循环过程**（直到收敛）:
   对于每个回合或每个时间步，执行以下步骤：
   
   - **状态观察**: 智能体观察当前的状态 \( s \)。
   
   - **选择动作**: 根据 ε-greedy 策略选择动作 \( a \)，即随机选择或选择最大 Q 值对应的动作。
   
   - **执行动作**: 在环境中执行动作 \( a \)，得到即时奖励 \( r \)，并观察新的状态 \( s' \)。
   
   - **更新 Q 值**: 根据贝尔曼方程更新 Q 值，具体公式如下：
     $$
     Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
     $$
     其中：
     - $\alpha$是学习率，控制 Q 值更新的步长。
     - $\gamma$ 是折扣因子，权衡当前奖励和未来奖励的重要性。
     - $\max_{a'} Q(s', a')$ 是在新状态 $s'$ 下的最优动作所对应的 Q 值。
   
   - **状态更新**: 将新的状态 $s'$ 作为当前状态，重复上述步骤。

3. **终止条件**: 该过程会一直重复，直到 Q 值收敛（即 Q 值变化趋于稳定）或达到预设的训练轮次。
  
  
### Q-learning的优点

1. **简单易实现**: Q-learning 是一个相对简单且易于实现的算法，它通过表格形式存储每个状态-动作对的 Q 值，适合离散状态和动作空间的问题。

2. **无模型方法（Model-Free）**: Q-learning 不需要知道环境的动态模型（如转移概率和奖励函数），它是一种基于经验的无模型学习方法，仅依赖智能体与环境的交互数据。

3. **全局最优**: 在理论上，Q-learning 能够在足够的时间和探索下收敛到最优策略，即找到最大化累积奖励的动作选择方式。
  
  
### Q-learning的局限性

1. **维数灾难**: 当状态空间和动作空间较大时（例如图像或高维度输入），Q-learning 需要维护一个巨大的 Q 值表，存储所有状态-动作对的值，这导致内存需求急剧增加，难以扩展到复杂问题。

2. **收敛速度慢**: 在某些复杂环境中，Q-learning 的收敛速度可能非常慢，尤其是在探索与利用不平衡的情况下。

3. **仅适用于离散动作空间**: 传统的 Q-learning 只适用于离散的状态和动作空间，对于连续动作空间，需要扩展算法或引入函数逼近方法（如深度 Q-learning）。
  
  
### 深度Q学习（Deep Q-Network, DQN）

为了解决 Q-learning 在高维状态空间下的扩展性问题，**深度 Q-learning（[[5.技能\AI\RL\_What_\DQN]]）** 被提出，它将**深度学习**与 Q-learning 结合，利用**深度神经网络**来近似 Q 值函数。DQN 在游戏领域（如 Atari 游戏）中表现出色，成功解决了高维图像输入的问题，推动了深度强化学习的发展。

DQN 的主要特点包括：
1. **使用深度神经网络**: 代替传统的 Q 值表，通过神经网络估计 Q 值。
2. **经验回放（Experience Replay）**: 通过存储过去的经验并随机采样用于训练，打破数据相关性，改善学习稳定性。
3. **目标网络（Target Network）**: 引入一个目标网络来计算目标 Q 值，减少网络更新过程中的不稳定性。
  
  
### 总结

**Q-learning** 是一种经典的强化学习算法，通过反复更新状态-动作值表来学习最优策略。它在简单、离散的环境中表现良好，但在高维状态空间或连续动作空间中，存在扩展性问题。通过引入深度学习技术，深度 Q-learning（DQN）使得 Q-learning 能够处理复杂的高维问题，极大地扩展了其应用范围。
  
  
## 代码示例

```python
from src.ai.rl.config.q_config import QConfig
from src.ai.rl.agent.agent_q import QAgent
from src.ai.rl.utils import rl_utils
from src.ai.rl.utils.rl_utils import *

matplotlib.use('Agg')


def train(cfg):
    env_train = gym.make(cfg.env_name, render_mode="rgb_array")
    agent = QAgent(cfg)

    for episode in range(cfg.episode_num):
        current_state = discretize_state(env_train.reset()[0])

        while True:
            action = agent.act(current_state)

            next_state, next_reward, done, _, _ = env_train.step(action)
            next_state = discretize_state(next_state)

            agent.replay_buffer.add(current_state, action, next_reward, next_state, done)
            agent.learn()

            current_state = next_state

            if done:
                break

        if (episode + 1) % 1 == 0:
            if (episode + 1) % 1 == 0:
                agent.save(episode, max_keep=3)
                predict(cfg, episode)

        # 每次更新网络后倾向于使用网络预测的 action 探索环境, 探索系数会越来越小
        if agent.cfg.epsilon > agent.cfg.epsilon_min:
            agent.cfg.epsilon = max(agent.cfg.epsilon_min, agent.cfg.epsilon * agent.cfg.epsilon_decay)

    env_train.close()


def predict(cfg, episode):
    cfg.is_train = False

    env_eval = gym.make(cfg.env_name, render_mode="human")
    agent = QAgent(cfg)

    reward_list = evaluate(env_eval, agent, episode)
    if (episode + 1) % 10 == 0 and len(reward_list) > 10:
        plot(reward_list, episode)


def evaluate(env_display, agent, episode):
    reward_list = []
    state = discretize_state(env_display.reset()[0])
    episode_reward = 0
    for t in range(10000):
        env_display.render()

        action = agent.predict(state)

        next_state, reward, done, _, _ = env_display.step(action)
        next_state = discretize_state(next_state)
        state = next_state

        episode_reward += reward

        if done:
            print(f"Episode {episode} finished after {t + 1} timesteps with total reward {episode_reward}")
            break
    reward_list.append(episode_reward)

    return reward_list


def plot(reward_list, episode=999):
    episodes_list = list(range(len(reward_list)))

    mv_return = rl_utils.moving_average(reward_list, 9)
    plt.plot(episodes_list, mv_return)
    plt.xlabel('Episodes')
    plt.ylabel('Returns')
    plt.title('Q on {}'.format(cfg.env_name))
    plt.show()

    plot_path = f'{FileUtils.get_project_path("output")}/q-{TimeUtils.get_cur_timestamp()}-{episode}.png'
    plt.savefig(plot_path)  # 保存图表为PNG文件
    plt.close()  # 关闭图表窗口，防止资源占用


# 离散化状态
def discretize_state(state, discretize_num=19):
    """https://github.com/openai/gym/wiki/CartPole-v0
    [position, cart velocity, angle, pole velocity]
    """
    lower_bounds = [-2.4, -3.0, -0.5, -2.0]
    upper_bounds = [2.4, 3.0, 0.5, 2.0]
    # ratios = [(state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(state))]
    # new_state = [int(round((discretize_num * ratios[i]))) for i in range(len(state))]
    # new_state = [min(discretize_num, max(0, new_state[i])) for i in range(len(state))]
    # return tuple(new_state)

    ratios = []
    new_state = []
    # 计算每个状态维度的归一化比例
    for i in range(len(state)):
        ratio = (state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i])
        ratios.append(ratio)

    # 离散化状态并四舍五入
    for i in range(len(state)):
        discretized_value = int(round(discretize_num * ratios[i]))
        new_state.append(discretized_value)

    # 边界调整
    for i in range(len(state)):
        new_state[i] = min(discretize_num - 1, max(0, new_state[i]))

    return tuple(new_state)


if __name__ == "__main__":
    args = Tools.parse_args()
    core = Core()
    core.init(env=args.env)
    Logger.instance().info('********************************* Q *********************************')

    cfg = QConfig()
    train(cfg)
    predict(cfg, 999)

```


**相关笔记:**

- [深度强化学习](../fb2736e62d44c8a1f1c732650a20795e18086c1b)
- [强化学习](../689f27ebabe459360038ee0e75643af26f2e718a)

{% pullquote mindmap mindmap-md %}
- 🔵
  - [深度强化学习](../fb2736e62d44c8a1f1c732650a20795e18086c1b)
  - [强化学习](../689f27ebabe459360038ee0e75643af26f2e718a)
{% endpullquote %}