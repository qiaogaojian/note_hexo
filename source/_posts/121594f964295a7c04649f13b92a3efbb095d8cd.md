---
title: AutoGen +Ollama  构建多智能体应用
date: 2024-08-22 20:01:04
categories: ['5.技能', 'AI', 'Agent']
tags: ['Agent', '技能', 'AI']
---

> 原文地址 [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/700107605)
  
  
## AutoGen 介绍

随着大语言模型（LLM）的发展，如何利用这些强大的语言处理能力构建智能体系统，以解决复杂任务，成为了一个值得探索的问题。微软研究团队研发了名叫 AutoGen 的多智能体对话框架，该框架通过设计可定制的智能体和对话编程范式，实现了利用 LLM 构建多样应用的目标。

GitHub：[https://github.com/microsoft/autogen](https://github.com/microsoft/autogen)

![](https://pic3.zhimg.com/v2-295eac6d916cca1a8e3b21903da15b26_r.jpg)

论文：[https://openreview.net/pdf?id=uAjxFFing2](https://openreview.net/pdf?id=uAjxFFing2)

![](https://pic4.zhimg.com/v2-0ad37b8485cc1fbb2b6b16f1afea86db_r.jpg)
  
  
### 简介

AutoGen 是一个开源框架，旨在通过将任务分解为多个智能体，让它们通过对话来协作完成，从而简化复杂的大语言模型应用的开发。该框架支持灵活的智能体设计、多智能体对话以及对话编程。AutoGen 框架已在多个领域取得成功应用，包括数学问题求解、检索增强问答、供应链优化、在线决策、娱乐等。

![](https://pic1.zhimg.com/v2-7c27581095d9b6a589e40678f70f4660_r.jpg)
  
  
### 架构

AutoGen 框架包含两个核心概念：可定制的智能体和对话编程。

![](https://pic4.zhimg.com/v2-4716f7dce68d09e1008adbd2857643c7_r.jpg)

可定制的智能体：AutoGen 支持设计多种类型的智能体，包括基于 LLM 的智能体、人类支持的智能体以及工具支持的智能体。每种智能体都具有特定的角色，可以进行多轮对话。AutoGen 还提供了多智能体组合的能力，可以实现更复杂的对话模式。

对话编程：AutoGen 采用对话编程范式，通过定义智能体的能力和角色，并编程它们之间的对话，实现了对复杂任务的分解和整合。AutoGen 提供了统一的对话接口，并支持自然语言和编程语言的混合控制，以实现灵活的对话流程。
  
  
### 应用

AutoGen 在多个领域实现了多智能体应用，包括数学问题求解、检索增强问答、供应链优化、在线决策、娱乐等。这些应用展示了 AutoGen 框架的灵活性和创新潜力，为智能体系统的开发提供了强有力的支持。

![](https://pic3.zhimg.com/v2-eef4c3f248cc4338e5b9006a1762e52a_r.jpg)

AutoGen 框架提供了一种简单、通用的多智能体对话方式，为利用 LLM 构建智能体系统提供了有力支持。AutoGen 框架在多个领域取得了成功应用，展现了其在构建智能体系统方面的巨大潜力。不过 AutoGen 在构建智能体应用尤其是多智能体应用的时候需要耗费大量的 token，这样本地大模型加速 / 服务平台 ollama 就成为了我这样穷人使用 Autogen 的好搭档。
  
  
## Ollama

ollama 是一个非常方便的 LLM 加速 / 服务化应用，我现在构建大模型应用，模型服务部分几乎用的都是它。ollama 的介绍及部署过程请参考：

[北方的郎：Linux 上部署 Ollama，启动 Mistral-7B 及 Gemma-7B 服务，测试效果](https://zhuanlan.zhihu.com/p/688811216)

[北方的郎：Ollama 下 LLM 的调用方式：post、langchain、lamaindex, 支持 openai api 方式](https://zhuanlan.zhihu.com/p/692360483)

[北方的郎：折腾 Ollama + CodeGPT 在 VSCode 中构建自己的本机智能开发助手](https://zhuanlan.zhihu.com/p/692106504)
  
  
## AutoGen + ollama 

新版的 ollama 已经支持 OpenAI 的 API 格式，参考：[https://ollama.com/blog/openai-compatibility](https://ollama.com/blog/openai-compatibility)

所以不需要通过 litellm 传一下了。

Install Autogen:[^1]  [^2]

```python
pip install pyautogen
```

然后执行代码即可。
  
  
### 3.1 单智能体

```python
from autogen import AssistantAgent, UserProxyAgent

config_list = [
  {
    "model": "qwen2:7b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "ollama",
  }
]

assistant = AssistantAgent("assistant", llm_config={"config_list": config_list})

user_proxy = UserProxyAgent("user_proxy", code_execution_config={"work_dir": "coding", "use_docker": False})
user_proxy.initiate_chat(assistant, message="做一个从哈尔滨去北京旅游5天的计划")
```
  
  
### 3.2 多智能体应用

```python  
  
### Code used:

import autogen

config_list_qwen = [
  {
    "model": "qwen2:7b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "ollama",
  }
]

config_list_codeqwen = [
  {
    "model": "deepseek-coder-v2:16b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "ollama",
  }
]

llm_config_qwen={
    "config_list": config_list_qwen,
}

llm_config_codeqwen={
    "config_list": config_list_codeqwen,
}

coder = autogen.AssistantAgent(    
    "coder",
    llm_config=llm_config_codeqwen
)

user_proxy = autogen.UserProxyAgent(    
    "user",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={"work_dir": "web", "use_docker": False},
    llm_config=llm_config_qwen,
    system_message="""Reply TERMINATE if the task has been solved at full satisfaction.
Otherwise, reply CONTINUE, or the reason why the task is not solved yet."""
)

task="""
开发一个python程序计算30以内的质数，然后 user_proxy agent 执行这个程序
"""

user_proxy.initiate_chat(coder, message=task)
```

然后看执行的效果，因为用的模型比较小，所以代码开发的有点问题，反复试了好几次：
  
  
## 参考

[^1]:  [GitHub - microsoft/autogen: A programming framework for agentic AI. Discord: https://aka.ms/autogen-dc. Roadmap: https://aka.ms/autogen-roadmap](https://github.com/microsoft/autogen)
[^2]: [reddit.com/r/AutoGenAI/comments/1b4tw1s/pyautogen\_vs\_autogen/](https://www.reddit.com/r/AutoGenAI/comments/1b4tw1s/pyautogen_vs_autogen/)