---
title: Ollama + continue 实现本地 github copilot
date: 2024-08-22 20:01:04
categories: ['5.技能', 'AI', 'Agent']
tags: ['Agent', 'AI', '技能']
---

> 原文地址 [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/686682108)

官方 github copilot 一年 100 美元，也就是 700 块。本人曾经薅一个月免费的试用过，自动补全代码，还能聊天询问看不懂的代码含义，确实很爽，但是价格实在伤不起。其他云厂商也提供了类似的工具，但是皆有 代码安全问题，泄露了公司代码，打工人也得跑路。

这么多开源的 LLM 模型，能否用开源的模型，玩自己的本地的 copilot 呢？下面我们就来解锁这个新姿势:

<mark style="background: #fefe00A6;">特别说明</mark> : **该方法对于电脑上没有 GPU 的童鞋也适用**。
  
  
### 第一步: 安装 ollama

ollama 地址: [Ollama](https://ollama.com/) [GitHub](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) [QA](https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored)

```python
# 配置环境变量: 
OLLAMA_MODELS=D:\AI\models
OLLAMA_ORIGINS=app://obsidian.md*
```
  
  
### 第二步: 通过 ollama 命令下载模型

```python
ollama run qwen2:7b           # 下载并运行  4.4G
/set parameter num_ctx 32000  # 设置上下文长度为32000个token
/save qwen2:7b                # 保存模型配置
/bye                          # 退出


# 模型推荐 (保存位置: D:/AI/models)  
ollama pull nomic-embed-text         # 文本转向量 274M
ollama pull mxbai-embed-large        # 文本转向量 669M
ollama pull starcoder2:3b            # 代码补全   1.7G
ollama pull deepseek-coder-v2:16b    # 代码补全   8.9G
ollama pull llava                    # 图片解释   4.7G  
```
支持的模型地址: [library](https://ollama.com/library)
配置模型存储位置: [[Ollama + Open WebUI 本地 LLM#自定义模型存储位置和环境变量（可选）]] [^1]

![](https://pic1.zhimg.com/v2-3a6ed0378e74e1e8dbf4b184cceaf098_r.jpg)

**下载过程中，如果发现网速太慢了，可以 ctrl +c 中断命令。然后重新执行 `ollama run codellama` 命令，可以断点续传。但是千万别关机，如果电脑关机，就只能从头开始了。**

**ollama 还支持很多其他的模型，如谷歌的 gemma、阿里的 Qwen、starcoder2 等**

![](https://pic2.zhimg.com/v2-c39389ca58346580a01be2a84e4fe3c5_r.jpg)![](https://pic1.zhimg.com/v2-42edca07188367e95305fbe94e2b4420_r.jpg)![](https://pic1.zhimg.com/v2-165dc49a99d05b31fddecb6663089068_r.jpg)
  
  
## 配置 Continue 插件 (推荐)

官网: [Continue](https://www.continue.dev/)
源码: [GitHub - continuedev/continue](https://github.com/continuedev/continue)
文档: [Continue](https://docs.continue.dev/setup/configuration)

同时有同学私信，上篇文章中介绍的 **cody ai 需要为每个项目配置 `.vscode/settings.json`，有没有一次配置，全局使用的方法？下面介绍的 continue 插件刚好满足这个要求**

安装插件

![](https://pic1.zhimg.com/v2-15f4ff7dd7441668a7bcf6586af45ef4_r.jpg)

配置插件

![](https://pic2.zhimg.com/v2-124a411cc782dfe9a84d98f6ee944185_r.jpg)

配置文件信息:

```python
{
  "models": [
    {
      "title": "ollama", # title随便写
      "model": "gemma", # model名
      "completionOptions": {},
      "apiBase": "http://127.0.0.1:11434",
      "provider": "ollama"
    }
  ],
  ...
  "tabAutocompleteModel": {
    "title": "gemma", # title随便写
    "provider": "ollama",
    "model": "gemma", # model名
    "apiBase": "http://127.0.0.1:11434"
  },
  ...
}
```

![](https://pic4.zhimg.com/v2-8ea93b298a341af629d00abc0cb7d063_r.jpg)

代码补全效果:

![](https://pic2.zhimg.com/v2-f4c086eeda80c2ef3fa98266f3edeaf1_r.jpg)

和 ollama 聊天:

![](https://pic2.zhimg.com/v2-44e55eca807d46b4120b5ebaf006ec29_r.jpg)

为代码添加注释:

![](https://pic2.zhimg.com/v2-93c210b5b25ed6f4addc020667128b3d_r.jpg)![](https://pic2.zhimg.com/v2-7abcd15b6f4558c503d04219bc7f0b5d_r.jpg)

vscode 中使用 ollama 其实还有很多插件，大家可以自己尝试: [https://github.com/ollama/ollama?tab=readme-ov-file#extensions--plugins](https://github.com/ollama/ollama?tab=readme-ov-file#extensions--plugins)

![](https://pic3.zhimg.com/v2-a90a6e9ee49a7345eb18dccdac48a7d6_r.jpg)
  
  
### 工程级别上下文

[Continue](https://docs.continue.dev/features/codebase-embeddings)

[Ollama](https://ollama.ai/) is the easiest way to get up and running with open-source language models. It provides an entirely local REST API for working with LLMs, including generating embeddings. We recommend using an embeddings model like `nomic-embed-text`:
```json
// ~/.continue/config.json
{
  "embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text",
    "apiBase": "http://localhost:11434" // optional, defaults to http://localhost:11434
  }
}
```
  
  
## 配置 Cody AI 插件

![](https://pic1.zhimg.com/v2-ad32acc61d166426850fb1aed217b4fc_r.jpg)

**通过 github 账号登录 cody AI，当然其他 google 账号也应该没问题**  [官网](https://sourcegraph.com/cody/manage) [教程](https://sourcegraph.com/docs/tutorials)

![](https://pic1.zhimg.com/v2-70ac2afcd4de11a4a78174f67d4d39c8_r.jpg)

**配置 cody AI**

![](https://pic2.zhimg.com/v2-5f2bd1e66f2866bbac09a66a930be515_r.jpg)

**修改 cody ai 的配置:** Cody › Autocomplete › Advanced:Provider

![](https://pic2.zhimg.com/v2-9f1a06f0d70b2020bb6070631132c3f1_r.jpg)

**修改项目目录下的 `.vscode/settings.json` 文件。** 这一步很重要，很多文章都缺少了这一步:

```python
    "cody.autocomplete.experimental.ollamaOptions": {
        "url": "http://127.0.0.1:11434",
        "model": "codellama"
    }
```

![](https://pic3.zhimg.com/v2-c651bd085255c48b3ebc712bd76586ae_r.jpg)

**此时在该目录下编码代码，就能自动补全了 (如果还不能, 可以重启下 vscode)**:

![](https://pic2.zhimg.com/v2-54f898faa54f5aaac26636b7c549397d_r.jpg)

和 cody ai 聊天，编码:

![](https://pic1.zhimg.com/v2-ff5a975ecccc4c8d247e79f4cbf03ea8_r.jpg)

  
  
## Ollama 插件

-   [Continue](https://github.com/continuedev/continue)
-   [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama)
-   [Obsidian BMO Chatbot plugin](https://github.com/longy2k/obsidian-bmo-chatbot)
-   [Obsidian Local GPT plugin](https://github.com/pfrankov/obsidian-local-gpt)
-   [Page Assist](https://github.com/n4ze3m/page-assist) (Chrome Extension)
-   [Copilot for Obsidian plugin](https://github.com/logancyang/obsidian-copilot)
-   [Llama Coder](https://github.com/ex3ndr/llama-coder) (Copilot alternative using Ollama)
-   [Ollama Copilot](https://github.com/bernardo-bruning/ollama-copilot) (Proxy that allows you to use ollama as a copilot like Github copilot)
-   [Raycast extension](https://github.com/MassimilianoPasquini97/raycast_ollama)
-   [Discollama](https://github.com/mxyng/discollama) (Discord bot inside the Ollama discord channel)
-   [Logseq Ollama plugin](https://github.com/omagdy7/ollama-logseq)
-   [NotesOllama](https://github.com/andersrex/notesollama) (Apple Notes Ollama plugin)
-   [Dagger Chatbot](https://github.com/samalba/dagger-chatbot)
-   [Discord AI Bot](https://github.com/mekb-turtle/discord-ai-bot)
-   [Ollama Telegram Bot](https://github.com/ruecat/ollama-telegram)
-   [Hass Ollama Conversation](https://github.com/ej52/hass-ollama-conversation)
-   [Rivet plugin](https://github.com/abrenneke/rivet-plugin-ollama)
-   [Cliobot](https://github.com/herval/cliobot) (Telegram bot with Ollama support)
-   [Open Interpreter](https://docs.openinterpreter.com/language-model-setup/local-models/ollama)
-   [twinny](https://github.com/rjmacarthy/twinny) (Copilot and Copilot chat alternative using Ollama)
-   [Wingman-AI](https://github.com/RussellCanfield/wingman-ai) (Copilot code and chat alternative using Ollama and Hugging Face)
-   [AI Telegram Bot](https://github.com/tusharhero/aitelegrambot) (Telegram bot using Ollama in backend)
-   [AI ST Completion](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (Sublime Text 4 AI assistant plugin with Ollama support)
-   [Discord-Ollama Chat Bot](https://github.com/kevinthedang/discord-ollama) (Generalized TypeScript Discord Bot w/ Tuning Documentation)
-   [Discord AI chat/moderation bot](https://github.com/rapmd73/Companion) Chat/moderation bot written in python. Uses Ollama to create personalities.
-   [Headless Ollama](https://github.com/nischalj10/headless-ollama) (Scripts to automatically install ollama client & models on any OS for apps that depends on ollama server)
  
  
## 参考

[^1]: [Can we change where the models are stored in windows · Issue #2551 · ollama/ollama · GitHub](https://github.com/ollama/ollama/issues/2551)

[Local Coding Assistant. Local-Coding-Assistant | by Akshay Dongare | Medium](https://medium.com/@akshayd02/local-coding-assistant-3faa2b6719be)